{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run gpt-oss with Hugging Face Transformers\n",
    "\n",
    "The Transformers library by Hugging Face provides a flexible way to load and run large language models locally or on a server. This guide will walk you through running [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) or [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) using Transformers, either with a high-level pipeline or via low-level `generate` calls with raw token IDs.\n",
    "\n",
    "We'll cover the use of [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) or [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) with the high-level pipeline abstraction, low-level `generate` calls, and serving models locally with `transformers serve`, in a way compatible with the Responses API.\n",
    "\n",
    "In this guide we'll run through various optimised ways to run the **gpt-oss models via Transformers.**\n",
    "\n",
    "**Bonus:** You can also fine-tune models via transformers, [check out our fine-tuning guide here](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick your model\n",
    "\n",
    "Both **gpt-oss** models are available on Hugging Face:\n",
    "\n",
    "- **`openai/gpt-oss-20b`**\n",
    "  - ~16GB VRAM requirement when using MXFP4\n",
    "  - Great for single high-end consumer GPUs\n",
    "- **`openai/gpt-oss-120b`**\n",
    "  - Requires ≥60GB VRAM or multi-GPU setup\n",
    "  - Ideal for H100-class hardware\n",
    "\n",
    "Both are **MXFP4 quantized** by default. Please, note that MXFP4 is supported in Hopper or later architectures. This includes data center GPUs such as H100 or GB200, as well as the latest RTX 50xx family of consumer cards.\n",
    "\n",
    "If you use `bfloat16` instead of MXFP4, memory consumption will be larger (~48 GB for the 20b parameter model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: The current version of HF Transformers has a glitch where an outdated torchvision dependency prevents transformers module from importing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick setup\n",
    "\n",
    "### 1. Install dependencies\n",
    "\n",
    "It's recommended to create a fresh Python environment. Install transformers, accelerate, as well as the Triton kernels for MXFP4 compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.56.1)\n",
      "Requirement already satisfied: kernels in /usr/local/lib/python3.11/dist-packages (0.10.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (77.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers kernels torch accelerate torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.56.1\n",
      "PyTorch version: 2.8.0+cu128\n",
      "Accelerate version: 1.10.1\n",
      "Selected device: cuda • Python 3.11.11\n",
      "CUDA devices: 1\n",
      "  GPU 0: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers, torch, accelerate, platform\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "dev = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(f\"Selected device: {dev} • Python {platform.python_version()}\")\n",
    "if dev == \"cuda\":\n",
    "    print(f\"CUDA devices: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (Optional) Enable multi-GPU\n",
    "\n",
    "If you're running large models, use Accelerate or torchrun to handle device mapping automatically.\n",
    "\n",
    "For this notebook, we'll use the `device_map=\"auto\"` parameter which automatically distributes the model across available GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an OpenAI Responses / Chat Completions endpoint\n",
    "\n",
    "To launch a server, simply use the `transformers serve` CLI command:\n",
    "\n",
    "```bash\n",
    "transformers serve\n",
    "```\n",
    "\n",
    "The simplest way to interact with the server is through the transformers chat CLI:\n",
    "\n",
    "```bash\n",
    "transformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\n",
    "```\n",
    "\n",
    "or by sending an HTTP request with cURL:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/v1/responses \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 0.9, \"max_tokens\": 1000, \"stream\": true, \"model\": \"openai/gpt-oss-20b\"}'\n",
    "```\n",
    "\n",
    "Additional use cases, like integrating `transformers serve` with Cursor and other tools, are detailed in [the documentation](https://huggingface.co/docs/transformers/main/serving)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick inference with pipeline\n",
    "\n",
    "The easiest way to run the gpt-oss models is with the Transformers high-level `pipeline` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0.dev20250319+cu128)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch==2.8.0->torchvision) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.8.0->torchvision) (2.1.5)\n",
      "Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.0.dev20250319+cu128\n",
      "    Uninstalling torchvision-0.22.0.dev20250319+cu128:\n",
      "      Successfully uninstalled torchvision-0.22.0.dev20250319+cu128\n",
      "Successfully installed torchvision-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3273\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3270\u001b[39m compiler = \u001b[38;5;28mself\u001b[39m.compile \u001b[38;5;28;01mif\u001b[39;00m shell_futures \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compiler_class()\n\u001b[32m   3272\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m3273\u001b[39m     cell_name = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3275\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.display_trap:\n\u001b[32m   3276\u001b[39m         \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[32m   3277\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/IPython/core/compilerop.py:155\u001b[39m, in \u001b[36mCachingCompiler.cache\u001b[39m\u001b[34m(self, transformed_code, number, raw_code)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raw_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    153\u001b[39m     raw_code = transformed_code\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m name = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_code_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Save the execution count\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m._filename_map[name] = number\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/ipykernel/compiler.py:105\u001b[39m, in \u001b[36mXCachingCompiler.get_code_name\u001b[39m\u001b[34m(self, raw_code, code, number)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_code_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_code, code, number):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the code name.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/ipykernel/compiler.py:91\u001b[39m, in \u001b[36mget_file_name\u001b[39m\u001b[34m(code)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cell_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     90\u001b[39m     name = murmur2_x86(code, get_tmp_hash_seed())\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     cell_name = \u001b[43mget_tmp_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m + os.sep + \u001b[38;5;28mstr\u001b[39m(name) + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cell_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/ipykernel/compiler.py:76\u001b[39m, in \u001b[36mget_tmp_directory\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tmp_directory\u001b[39m():\n\u001b[32m     75\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a temp directory.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     tmp_dir = convert_to_long_pathname(\u001b[43mtempfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     77\u001b[39m     pid = os.getpid()\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_dir + os.sep + \u001b[33m\"\u001b[39m\u001b[33mipykernel_\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(pid)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/tempfile.py:455\u001b[39m, in \u001b[36mgettempdir\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgettempdir\u001b[39m():\n\u001b[32m    454\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns tempfile.tempdir as str.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _os.fsdecode(\u001b[43m_gettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/tempfile.py:448\u001b[39m, in \u001b[36m_gettempdir\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tempdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m         tempdir = \u001b[43m_get_default_tempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    450\u001b[39m     _once_lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/tempfile.py:363\u001b[39m, in \u001b[36m_get_default_tempdir\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    362\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# no point trying more names in this directory\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(_errno.ENOENT,\n\u001b[32m    364\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mNo usable temporary directory found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m    365\u001b[39m                         dirlist)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/']"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation with the model using the chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Be concise.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "print(\"Generated response:\")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced inference with `.generate()`\n",
    "\n",
    "If you want more control, you can load the model and tokenizer manually and invoke the `.generate()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example generation with manual control\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    "\n",
    "# Apply chat template and tokenize\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "print(f\"Input tokens shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the full response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nFull response:\")\n",
    "print(full_response)\n",
    "\n",
    "# Extract only the generated part\n",
    "input_length = inputs['input_ids'].shape[-1]\n",
    "generated_response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "print(\"\\nGenerated response only:\")\n",
    "print(generated_response)\n",
    "\n",
    "# Decode and extract only the assistant's final content (Harmony-style)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "final_marker = \"<|assistant|><|final|>\"\n",
    "end_marker = \"<|end|>\"\n",
    "if final_marker in text:\n",
    "    text = text.split(final_marker, 1)[1]\n",
    "if end_marker in text:\n",
    "    text = text.split(end_marker, 1)[0]\n",
    "print(\"Final:\", text.strip()[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31988a1",
   "metadata": {},
   "source": [
    "## Streaming tokens (prints only assistant **final**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, sys\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "def stream_final_only(model, tokenizer, messages, generate_kwargs):\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=False)\n",
    "\n",
    "    t = threading.Thread(target=model.generate, kwargs=dict(input_ids=inputs, streamer=streamer, **generate_kwargs))\n",
    "    t.start()\n",
    "\n",
    "    buf, printing_final = \"\", False\n",
    "    final_token, end_token = \"<|assistant|><|final|>\", \"<|end|>\"\n",
    "    for piece in streamer:\n",
    "        buf += piece\n",
    "        if not printing_final and final_token in buf:\n",
    "            printing_final = True\n",
    "            out = buf.split(final_token, 1)[1].replace(end_token, \"\")\n",
    "            sys.stdout.write(out); sys.stdout.flush()\n",
    "            buf = \"\"\n",
    "        elif printing_final:\n",
    "            sys.stdout.write(piece.replace(end_token, \"\")); sys.stdout.flush()\n",
    "    t.join()\n",
    "\n",
    "messages2 = [\n",
    "    {\"role\": \"system\", \"content\": \"Answer briefly.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What’s the difference between analysis and final channels?\"}\n",
    "]\n",
    "gen = dict(max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "stream_final_only(model, tokenizer, messages2, gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat template and tool calling\n",
    "\n",
    "OpenAI gpt-oss models use the [harmony response format](https://cookbook.openai.com/article/harmony) for structuring messages, including reasoning and tool calls.\n",
    "\n",
    "To construct prompts you can use the built-in chat template of Transformers. Alternatively, you can install and use the [openai-harmony library](https://github.com/openai/harmony) for more control.\n",
    "\n",
    "### Using the built-in chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with system prompt and chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Always respond in riddles\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "# Generate with the chat template\n",
    "generated = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Extract only the assistant's response\n",
    "response = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(\"Assistant's riddle response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the openai-harmony library\n",
    "\n",
    "For more advanced control over the conversation format, you can use the openai-harmony library. \n",
    "\n",
    "First, install it:\n",
    "```bash\n",
    "pip install openai-harmony\n",
    "```\n",
    "\n",
    "**Note:** The following cell demonstrates the harmony library usage, but may require the actual library to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using openai-harmony library (requires installation)\n",
    "# Uncomment and run if you have openai-harmony installed\n",
    "\n",
    "'''\n",
    "import json\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName,\n",
    "    load_harmony_encoding,\n",
    "    Conversation,\n",
    "    Message,\n",
    "    Role,\n",
    "    SystemContent,\n",
    "    DeveloperContent\n",
    ")\n",
    "\n",
    "# Load harmony encoding\n",
    "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "\n",
    "# Build conversation\n",
    "convo = Conversation.from_messages([\n",
    "    Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),\n",
    "    Message.from_role_and_content(\n",
    "        Role.DEVELOPER,\n",
    "        DeveloperContent.new().with_instructions(\"Always respond in riddles\")\n",
    "    ),\n",
    "    Message.from_role_and_content(Role.USER, \"What is the weather like in SF?\")\n",
    "])\n",
    "\n",
    "# Render prompt\n",
    "prefill_ids = encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\n",
    "stop_token_ids = encoding.stop_tokens_for_assistant_actions()\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    input_ids=[prefill_ids],\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=stop_token_ids\n",
    ")\n",
    "\n",
    "# Parse completion tokens\n",
    "completion_ids = outputs[0][len(prefill_ids):]\n",
    "entries = encoding.parse_messages_from_completion_tokens(completion_ids, Role.ASSISTANT)\n",
    "\n",
    "for message in entries:\n",
    "    print(json.dumps(message.to_dict(), indent=2))\n",
    "'''\n",
    "\n",
    "print(\"Harmony library example code shown above (commented out)\")\n",
    "print(\"Note: The Developer role in Harmony maps to the system prompt in the chat template.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU & distributed inference\n",
    "\n",
    "The large gpt-oss-120b fits on a single H100 GPU when using MXFP4. If you want to run it on multiple GPUs, you can:\n",
    "\n",
    "- Use `tp_plan=\"auto\"` for automatic placement and tensor parallelism\n",
    "- Launch with `accelerate launch` or `torchrun` for distributed setups\n",
    "- Leverage Expert Parallelism\n",
    "- Use specialised Flash attention kernels for faster inference\n",
    "\n",
    "### Example multi-GPU setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU inference example (requires multiple GPUs)\n",
    "# This cell demonstrates the configuration but may not run on single GPU systems\n",
    "\n",
    "'''\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.distributed import DistributedConfig\n",
    "import torch\n",
    "\n",
    "model_path = \"openai/gpt-oss-120b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "\n",
    "device_map = {\n",
    "    # Enable Expert Parallelism\n",
    "    \"distributed_config\": DistributedConfig(enable_expert_parallel=1),\n",
    "    # Enable Tensor Parallelism\n",
    "    \"tp_plan\": \"auto\",\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    **device_map,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "     {\"role\": \"user\", \"content\": \"Explain how expert parallelism works in large language models.\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(\"Model response:\", response.split(\"<|channel|>final<|message|>\")[-1].strip())\n",
    "'''\n",
    "\n",
    "print(\"Multi-GPU setup example shown above (commented out)\")\n",
    "print(\"\\nTo run this on a node with four GPUs, use:\")\n",
    "print(\"torchrun --nproc_per_node=4 your_script.py\")\n",
    "\n",
    "# Show current GPU configuration\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCurrent setup:\")\n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\\nNo CUDA GPUs available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples and Tips\n",
    "\n",
    "### Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        cached = torch.cuda.memory_reserved(i) / 1e9\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        print(f\"  GPU {i}: {allocated:.1f}GB allocated, {cached:.1f}GB cached, {total:.1f}GB total\")\n",
    "\n",
    "# Clear cache if needed\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processing multiple prompts\n",
    "batch_messages = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Explain quantum computing.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"What is the future of AI?\"}]\n",
    "]\n",
    "\n",
    "print(\"Processing batch of prompts...\")\n",
    "for i, messages in enumerate(batch_messages):\n",
    "    print(f\"\\n--- Prompt {i+1} ---\")\n",
    "    print(f\"Input: {messages[0]['content']}\")\n",
    "\n",
    "    # You can use either the pipeline or manual generation here\n",
    "    # Using pipeline for simplicity:\n",
    "    if 'generator' in locals():\n",
    "        result = generator(\n",
    "            messages,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        print(f\"Output: {result[0]['generated_text'][-200:]}...\")  # Show last 200 chars\n",
    "    else:\n",
    "        print(\"Generator not available - run the pipeline example first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated various ways to run OpenAI's gpt-oss models using Hugging Face Transformers:\n",
    "\n",
    "1. **Quick setup** with required dependencies\n",
    "2. **Pipeline API** for simple, high-level inference\n",
    "3. **Manual generation** with `.generate()` for more control\n",
    "4. **Chat templates** for conversation-style interactions\n",
    "5. **Harmony library integration** for advanced message formatting\n",
    "6. **Multi-GPU configurations** for large-scale inference\n",
    "\n",
    "### Key takeaways:\n",
    "- Start with the pipeline API for quick experimentation\n",
    "- Use manual tokenization and generation for production deployments\n",
    "- Consider MXFP4 quantization for memory efficiency on compatible hardware\n",
    "- Leverage multi-GPU setups for the larger 120B model\n",
    "- Use proper chat templates for conversation-style applications\n",
    "\n",
    "### Next steps:\n",
    "- Explore fine-tuning capabilities\n",
    "- Set up serving endpoints for production use\n",
    "- Experiment with different sampling strategies\n",
    "- Integrate with your specific use case or application\n",
    "\n",
    "For more advanced topics, check out the [OpenAI Cookbook](https://cookbook.openai.com) and [Hugging Face Transformers documentation](https://huggingface.co/docs/transformers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
